{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1adec87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv2D, Dense, BatchNormalization, Activation, Dropout, MaxPooling2D, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d907a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/kunal/OneDrive/Desktop/Expression Dataset/train/'\n",
    "test_dir = 'C:/Users/kunal/OneDrive/Desktop/Expression Dataset/test/'\n",
    "\n",
    "row = 48\n",
    "col = 48\n",
    "classes = len(os.listdir('C:/Users/kunal/OneDrive/Desktop/Expression Dataset/train/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3bc54c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set :\n",
      "angry folder contains\t\t 3995 image\n",
      "disgust folder contains\t\t 436 image\n",
      "fear folder contains\t\t 4097 image\n",
      "happy folder contains\t\t 7215 image\n",
      "neutral folder contains\t\t 4965 image\n",
      "sad folder contains\t\t 4830 image\n",
      "surprise folder contains\t\t 3171 image\n",
      "\n",
      "Test Set :\n",
      "angry folder contains\t\t 958 images\n",
      "disgust folder contains\t\t 111 images\n",
      "fear folder contains\t\t 1024 images\n",
      "happy folder contains\t\t 1774 images\n",
      "neutral folder contains\t\t 1233 images\n",
      "sad folder contains\t\t 1247 images\n",
      "surprise folder contains\t\t 831 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set :\")\n",
    "\n",
    "train_count = []\n",
    "for folder in os.listdir(train_dir) :\n",
    "    print(folder, \"folder contains\\t\\t\", len(os.listdir(train_dir+folder)), \"image\")\n",
    "    train_count.append(len(os.listdir(train_dir+folder)))\n",
    "    \n",
    "print()\n",
    "\n",
    "test_count = []\n",
    "print(\"Test Set :\")\n",
    "for folder in os.listdir(test_dir) :\n",
    "    print(folder, \"folder contains\\t\\t\", len(os.listdir(test_dir+folder)), \"images\")\n",
    "    test_count.append(len(os.listdir(test_dir+folder)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a41e879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   zoom_range=0.3,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_dir,\n",
    "                                                batch_size=64,\n",
    "                                                target_size=(48,48),\n",
    "                                                shuffle=True,\n",
    "                                                color_mode='grayscale',\n",
    "                                                class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_set = test_datagen.flow_from_directory(test_dir,\n",
    "                                                batch_size=64,\n",
    "                                                target_size=(48,48),\n",
    "                                                shuffle=True,\n",
    "                                                color_mode='grayscale',\n",
    "                                                class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b27978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprised']\n",
    "\n",
    "# Assuming you have a generator object called 'train_generator'\n",
    "img, label = next(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eef4902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "\n",
    "i=random.randint(0, (img.shape[0])-1)\n",
    "image = img[i]\n",
    "labl = class_labels[label[i].argmax()]\n",
    "plt.imshow(image[:,:,0], cmap='gray')\n",
    "plt.title(labl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b6fe1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 48, 48, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 24, 24, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 12, 12, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               4719104   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5111815 (19.50 MB)\n",
      "Trainable params: 5111175 (19.50 MB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(48,48,1)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3),padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3),padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b022d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_path = 'ferNet.h5'\n",
    "log_dir = \"checkpoint/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=chk_path,\n",
    "                             save_best_only=True,\n",
    "                             verbose=1,\n",
    "                             mode='min',\n",
    "                             moniter='val_accuracy')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', \n",
    "                          min_delta=0, \n",
    "                          patience=3, \n",
    "                          verbose=1, \n",
    "                          restore_best_weights=True)\n",
    "                        \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                              factor=0.2, \n",
    "                              patience=6, \n",
    "                              verbose=1, \n",
    "                              min_delta=0.0001)\n",
    "\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "callbacks = [checkpoint, reduce_lr, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64bba8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 2.9945 - accuracy: 0.2777\n",
      "Epoch 1: val_loss improved from inf to 7.11064, saving model to ferNet.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kunal\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 997s 2s/step - loss: 2.9945 - accuracy: 0.2777 - val_loss: 7.1106 - val_accuracy: 0.1878 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.8191 - accuracy: 0.3434\n",
      "Epoch 2: val_loss improved from 7.11064 to 1.77358, saving model to ferNet.h5\n",
      "448/448 [==============================] - 485s 1s/step - loss: 1.8191 - accuracy: 0.3434 - val_loss: 1.7736 - val_accuracy: 0.3417 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.6988 - accuracy: 0.3796\n",
      "Epoch 3: val_loss improved from 1.77358 to 1.70322, saving model to ferNet.h5\n",
      "448/448 [==============================] - 484s 1s/step - loss: 1.6988 - accuracy: 0.3796 - val_loss: 1.7032 - val_accuracy: 0.3898 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.6559 - accuracy: 0.4097\n",
      "Epoch 4: val_loss improved from 1.70322 to 1.57615, saving model to ferNet.h5\n",
      "448/448 [==============================] - 484s 1s/step - loss: 1.6559 - accuracy: 0.4097 - val_loss: 1.5761 - val_accuracy: 0.4427 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.6233 - accuracy: 0.4290\n",
      "Epoch 5: val_loss improved from 1.57615 to 1.52037, saving model to ferNet.h5\n",
      "448/448 [==============================] - 485s 1s/step - loss: 1.6233 - accuracy: 0.4290 - val_loss: 1.5204 - val_accuracy: 0.4672 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5824 - accuracy: 0.4452\n",
      "Epoch 6: val_loss did not improve from 1.52037\n",
      "448/448 [==============================] - 485s 1s/step - loss: 1.5824 - accuracy: 0.4452 - val_loss: 1.5445 - val_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5442 - accuracy: 0.4613\n",
      "Epoch 7: val_loss improved from 1.52037 to 1.47696, saving model to ferNet.h5\n",
      "448/448 [==============================] - 484s 1s/step - loss: 1.5442 - accuracy: 0.4613 - val_loss: 1.4770 - val_accuracy: 0.4909 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5306 - accuracy: 0.4715\n",
      "Epoch 8: val_loss improved from 1.47696 to 1.40864, saving model to ferNet.h5\n",
      "448/448 [==============================] - 484s 1s/step - loss: 1.5306 - accuracy: 0.4715 - val_loss: 1.4086 - val_accuracy: 0.5126 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5137 - accuracy: 0.4716\n",
      "Epoch 9: val_loss did not improve from 1.40864\n",
      "448/448 [==============================] - 483s 1s/step - loss: 1.5137 - accuracy: 0.4716 - val_loss: 1.4231 - val_accuracy: 0.5014 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5001 - accuracy: 0.4775\n",
      "Epoch 10: val_loss improved from 1.40864 to 1.38722, saving model to ferNet.h5\n",
      "448/448 [==============================] - 478s 1s/step - loss: 1.5001 - accuracy: 0.4775 - val_loss: 1.3872 - val_accuracy: 0.5137 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4845 - accuracy: 0.4848\n",
      "Epoch 11: val_loss did not improve from 1.38722\n",
      "448/448 [==============================] - 713s 2s/step - loss: 1.4845 - accuracy: 0.4848 - val_loss: 1.5716 - val_accuracy: 0.4941 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4738 - accuracy: 0.4917\n",
      "Epoch 12: val_loss improved from 1.38722 to 1.35135, saving model to ferNet.h5\n",
      "448/448 [==============================] - 546s 1s/step - loss: 1.4738 - accuracy: 0.4917 - val_loss: 1.3513 - val_accuracy: 0.5335 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4683 - accuracy: 0.4926\n",
      "Epoch 13: val_loss did not improve from 1.35135\n",
      "448/448 [==============================] - 583s 1s/step - loss: 1.4683 - accuracy: 0.4926 - val_loss: 1.3581 - val_accuracy: 0.5272 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4515 - accuracy: 0.4951\n",
      "Epoch 14: val_loss did not improve from 1.35135\n",
      "448/448 [==============================] - 516s 1s/step - loss: 1.4515 - accuracy: 0.4951 - val_loss: 1.3570 - val_accuracy: 0.5338 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4509 - accuracy: 0.4949\n",
      "Epoch 15: val_loss did not improve from 1.35135\n",
      "448/448 [==============================] - 480s 1s/step - loss: 1.4509 - accuracy: 0.4949 - val_loss: 1.3517 - val_accuracy: 0.5364 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4465 - accuracy: 0.5009\n",
      "Epoch 16: val_loss improved from 1.35135 to 1.34986, saving model to ferNet.h5\n",
      "448/448 [==============================] - 481s 1s/step - loss: 1.4465 - accuracy: 0.5009 - val_loss: 1.3499 - val_accuracy: 0.5301 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4281 - accuracy: 0.5041\n",
      "Epoch 17: val_loss did not improve from 1.34986\n",
      "448/448 [==============================] - 482s 1s/step - loss: 1.4281 - accuracy: 0.5041 - val_loss: 1.3504 - val_accuracy: 0.5319 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4353 - accuracy: 0.4991\n",
      "Epoch 18: val_loss did not improve from 1.34986\n",
      "448/448 [==============================] - 485s 1s/step - loss: 1.4353 - accuracy: 0.4991 - val_loss: 1.3756 - val_accuracy: 0.5251 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4243 - accuracy: 0.5076\n",
      "Epoch 19: val_loss improved from 1.34986 to 1.31987, saving model to ferNet.h5\n",
      "448/448 [==============================] - 483s 1s/step - loss: 1.4243 - accuracy: 0.5076 - val_loss: 1.3199 - val_accuracy: 0.5438 - lr: 0.0010\n",
      "Epoch 20/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4216 - accuracy: 0.5088\n",
      "Epoch 20: val_loss did not improve from 1.31987\n",
      "448/448 [==============================] - 483s 1s/step - loss: 1.4216 - accuracy: 0.5088 - val_loss: 1.3299 - val_accuracy: 0.5395 - lr: 0.0010\n",
      "Epoch 21/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4033 - accuracy: 0.5142\n",
      "Epoch 21: val_loss did not improve from 1.31987\n",
      "448/448 [==============================] - 484s 1s/step - loss: 1.4033 - accuracy: 0.5142 - val_loss: 1.4017 - val_accuracy: 0.5257 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4083 - accuracy: 0.5146\n",
      "Epoch 22: val_loss improved from 1.31987 to 1.29407, saving model to ferNet.h5\n",
      "448/448 [==============================] - 485s 1s/step - loss: 1.4083 - accuracy: 0.5146 - val_loss: 1.2941 - val_accuracy: 0.5601 - lr: 0.0010\n",
      "Epoch 23/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.3975 - accuracy: 0.5191\n",
      "Epoch 23: val_loss did not improve from 1.29407\n",
      "448/448 [==============================] - 484s 1s/step - loss: 1.3975 - accuracy: 0.5191 - val_loss: 1.3084 - val_accuracy: 0.5485 - lr: 0.0010\n",
      "Epoch 24/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.3964 - accuracy: 0.5201\n",
      "Epoch 24: val_loss did not improve from 1.29407\n",
      "448/448 [==============================] - 484s 1s/step - loss: 1.3964 - accuracy: 0.5201 - val_loss: 1.2966 - val_accuracy: 0.5487 - lr: 0.0010\n",
      "Epoch 25/25\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.3927 - accuracy: 0.5166\n",
      "Epoch 25: val_loss did not improve from 1.29407\n",
      "448/448 [==============================] - 484s 1s/step - loss: 1.3927 - accuracy: 0.5166 - val_loss: 1.3145 - val_accuracy: 0.5413 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = training_set.n // training_set.batch_size\n",
    "validation_steps = test_set.n // test_set.batch_size\n",
    "\n",
    "hist = model.fit(x=training_set,\n",
    "                  validation_data=test_set,\n",
    "                  epochs=25,\n",
    "                  callbacks=callbacks,\n",
    "                  steps_per_epoch=steps_per_epoch,\n",
    "                  validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6269e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_trained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8276fdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Detected Emotion: Sad\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "home_folder = 'C:/Users/kunal/OneDrive/Desktop/Expression Dataset'\n",
    "test_folder = os.path.join(home_folder, 'test')\n",
    "train_folder = os.path.join(home_folder, 'train')\n",
    "\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".JPG\"):\n",
    "            img = Image.open(os.path.join(folder_path, filename))\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "    return images\n",
    "\n",
    "test_images = load_images_from_folder(test_folder)\n",
    "train_images = load_images_from_folder(train_folder)\n",
    "\n",
    "test_data = np.array([np.array(img) for img in test_images])\n",
    "train_data = np.array([np.array(img) for img in train_images])\n",
    "\n",
    "# Loading trained model\n",
    "my_model = load_model('model_trained.h5', compile=False)\n",
    "\n",
    "# Define the emotion labels\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprised']\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(0)  \n",
    "frame_count = 0  # Counter for frames\n",
    "detected_emotion = None  \n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray[y:y + h, x:x + w]\n",
    "        face_img = cv2.resize(face_roi, (48, 48))\n",
    "        face_img = face_img / 255.0  # Normalize\n",
    "\n",
    "        # Make predictions using your model\n",
    "        predictions = my_model.predict(np.array([face_img.reshape(48, 48, 1)]))\n",
    "        detected_emotion = class_labels[np.argmax(predictions)]\n",
    "\n",
    "        # Draw a rectangle around the detected face and display the emotion\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, detected_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"webcam\", frame)\n",
    "    \n",
    "    if frame_count == 25:  # Close the webcam after 1 seconds (assuming 25 frames per second)\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# Release the camera\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Access the detected emotion after the webcam closes\n",
    "print(\"Detected Emotion:\", detected_emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "542c08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "mood_music=pd.read_csv(r\"C:\\Users\\kunal\\Downloads\\archive (3)\\data_moods.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84e4c01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>artist</th>\n",
       "      <th>mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999</td>\n",
       "      <td>Prince</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>Blonde Redhead</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9 Crimes</td>\n",
       "      <td>Damien Rice</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99 Luftballons</td>\n",
       "      <td>Nena</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Boy Brushed Red Living In Black And White</td>\n",
       "      <td>Underoath</td>\n",
       "      <td>Energetic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          name          artist       mood\n",
       "0                                         1999          Prince      Happy\n",
       "1                                           23  Blonde Redhead        Sad\n",
       "2                                     9 Crimes     Damien Rice        Sad\n",
       "3                               99 Luftballons            Nena      Happy\n",
       "4  A Boy Brushed Red Living In Black And White       Underoath  Energetic"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mood_music=mood_music[['name', 'artist', 'mood']]\n",
    "mood_music.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7951aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>artist</th>\n",
       "      <th>mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>481</td>\n",
       "      <td>Simple Song</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>598</td>\n",
       "      <td>Unmade</td>\n",
       "      <td>Thom Yorke</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Andromeda</td>\n",
       "      <td>Weyes Blood</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>459</td>\n",
       "      <td>Sad Corny Fuck</td>\n",
       "      <td>JP Saxe</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>669</td>\n",
       "      <td>blue</td>\n",
       "      <td>Kamal.</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index            name       artist mood\n",
       "0    481     Simple Song    Passenger  Sad\n",
       "1    598          Unmade   Thom Yorke  Sad\n",
       "2     30       Andromeda  Weyes Blood  Sad\n",
       "3    459  Sad Corny Fuck      JP Saxe  Sad\n",
       "4    669            blue       Kamal.  Sad"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#selecting music according to moods\n",
    "\n",
    "if (detected_emotion=='Angry' or detected_emotion=='Disgust' or detected_emotion=='Fear'):\n",
    "    filter1=mood_music['mood']=='Calm' #Filtering according to 'mood' column in dataset\n",
    "    f1=mood_music.where(filter1)\n",
    "    f1=f1.dropna() #dropping rows that are empty\n",
    "    f2=f1.sample(n=5) #Displaying 5 songs\n",
    "    f2.reset_index(inplace=True)\n",
    "    display(f2)\n",
    "if (detected_emotion=='Happy' or detected_emotion=='Neutral'):\n",
    "    filter1=mood_music['mood']=='Happy' #Filtering according to 'mood' column in dataset\n",
    "    f1=mood_music.where(filter1)\n",
    "    f1=f1.dropna() #dropping rows that are empty\n",
    "    f2=f1.sample(n=5) #Displaying 5 songs\n",
    "    f2.reset_index(inplace=True)\n",
    "    display(f2)\n",
    "    \n",
    "if (detected_emotion=='Sad'):\n",
    "    filter1=mood_music['mood']=='Sad' #Filtering according to 'mood' column in dataset\n",
    "    f1=mood_music.where(filter1)\n",
    "    f1=f1.dropna() #dropping rows that are empty\n",
    "    f2=f1.sample(n=5) #Displaying 5 songs\n",
    "    f2.reset_index(inplace=True)\n",
    "    display(f2)\n",
    "    \n",
    "if (detected_emotion=='Surprised'):\n",
    "    filter1=mood_music['mood']=='Energetic' #Filtering according to 'mood' column in dataset\n",
    "    f1=mood_music.where(filter1)\n",
    "    f1=f1.dropna() #dropping rows that are empty\n",
    "    f2=f1.sample(n=5) #Displaying 5 songs\n",
    "    f2.reset_index(inplace=True)\n",
    "    display(f2)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
